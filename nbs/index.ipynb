{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "# Put any special code here e.g. `! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobs VAULT\n",
    " \n",
    "> **jacobs-vault** is Jacobs' response to the VAULT quick-turn challenge with AIS shipping & TLE satellite data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAULT converts AIS ship records into clickable tracks that show satellite coverage for the location clicked. The demo shows both Jupyter maps (via iPyLeaflet) and Google-Earth style maps (using OpenSphere).  \n",
    "\n",
    "<img src=\"images/vault-demo-3.gif\" />\n",
    "\n",
    "For demo purposes, we treat all entries in the TLE file as viable satellites. In reality most of these are space junk, but within 10 years there could be over 10,000 cubesats. \n",
    "\n",
    "The project also contains scripts & notebooks to ingest, characterize, and clean the AIS & TLE data, detect outliers, and cluster tracks. \n",
    "\n",
    "<table>\n",
    "    <tr style=\"background-color:#FFFFFF\">\n",
    "        <td style=\"background-color:#FFFFFF\">\n",
    "<img src=\"images/Jacobs_logo_rgb_black.svg\" width=\"200\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "<img alt=\"Satellites Visible\" src=\"images/starmap_new.png\" width=\"300\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# NOTE: the <img> tags above must be zero-indented due to a bug in nbdev!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "Further documents are hosted on [GitHub Pages](https://cmorris-jacobs.github.io/jacobs-vault/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing\n",
    "\n",
    "These instructions require Docker [(obtained here)](https://docs.docker.com/get-docker/) and a DockerHub account [(obtained here)](https://hub.docker.com/). \n",
    "\n",
    "Once docker is installed and the DockerHub account is established, in your operating system command line run:\n",
    "```bash\n",
    "docker login\n",
    "docker pull ke2jacobs/jacobs-vault-nb-with-data:latest\n",
    "docker run --rm -it -p 2080:2080 ke2jacobs/jacobs-vault-nb-with-data:latest\n",
    "```\n",
    "\n",
    "The container will present a message:\n",
    "```\n",
    "To access the notebook, open this file in a browser:         \n",
    "file:///root/.local/share/jupyter/runtime/nbserver-1-open.html    \n",
    "Or copy and paste one of these URLs:       \n",
    "http://d20796a21c64:2080/?token=455d272e289a90dbc2533de2cb7ddec9a5574dc3fac5ef66\n",
    "     or     \n",
    "http://127.0.0.1:2080/?token=455d272e289a90dbc2533de2cb7ddec9a5574dc3fac5ef66\n",
    "```\n",
    "Open a browser window and paste the URL into the browser (http://127.0.0.1: â€¦ is usually the best choice)\n",
    "\n",
    "The main Jupyter page will be displayed.\n",
    " \n",
    "* Click on the folder named \"demo\"\n",
    "* Click on the file named \"VAULT_Demo.ipynb\"\n",
    "* Click on the \"Cell\" menu item and choose \"Run All\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributing and Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore further, \n",
    "\n",
    "1. Clone the repository with ...\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/cmorris-jacobs/jacobs-vault\n",
    "cd jacobs-vault\n",
    "``` \n",
    "\n",
    "2. Obtain a copy of the VAULT data, and put it in `jacobs-vault/data`, for example with :\n",
    "```bash\n",
    "ln -s path/to/data data\n",
    "```\n",
    "\n",
    "3. Using either conda or pip, create the vault virtual environment so you have the required packages. Using conda, that would be:\n",
    "```bash\n",
    "conda create -f environment.yml\n",
    "```\n",
    "Wait while it installs packages...\n",
    "\n",
    "4. Activate the vault virtual environment.\n",
    "```bash\n",
    "conda activate vault\n",
    "```\n",
    "\n",
    "5. To run the **demo** from here:\n",
    "```bash\n",
    "cd demo\n",
    "ln -s ../data ./\n",
    ". run.sh\n",
    "```\n",
    "The `ln` line just makes ETL'd data visible from `demo/data`. Use other techniques if you prefer.\n",
    "\n",
    "5. If you plan to be pushing to git, then use nbdev to install git hooks - mostly relevant to notebooks in `nbs/`:\n",
    "```bash\n",
    "nbdev_install_git_hooks\n",
    "```\n",
    "If you will be modifying notebooks in that folder, familiarize yourself with fastai's nbdev, and do a `make` before commit/push to update associated modules and docs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jacobs-VAULT is the result of a hackathon challenge, so in addition to a working demo and analysis notebooks, it still has exploratory paths and alternate approaches. Folders are in three rough groups:\n",
    "\n",
    "### ETL Folders\n",
    "* `etl` - Original ETL scripts, mostly Spark SQL and Hive.\n",
    "* `ais-analytics` - Subproject Spark to analyze AIS data. Alternate ETL.\n",
    "* `geotransformer` - Subproject using Spark to analyze TLE data. Alternate ETL. Directly calls `sgp4` and the `astropy` package, instead of `skyfield`.\n",
    "\n",
    "### nbdev Folders\n",
    "A mix of exploratory notebooks and literate programming notebooks that generate Python modules and documentation (including this README) via the `nbdev` package. Controlled by the toplevel `Makefile`, using the `vault` virtual environment captured in `environment.yml`.  \n",
    "* `nbs` - Toplevel notebooks, generate docs, modules, and README.\n",
    "* `jacobs_vault` - Python modules generated from `nbs/` by `nbdev` package\n",
    "* `docs` - Documentation generated from `nbs/` by `nbdev` package\n",
    "* `data` - (See \"Demo Folders\".)\n",
    "\n",
    "To run the notebooks in the `nbs/` folder, \n",
    "activate the `vault` Python virtual environment and start a new jupyter kernel.\n",
    "```bash\n",
    "conda env -f environment.yml\n",
    "conda activate vault\n",
    "jupyter notebook\n",
    "```\n",
    "That should start a jupyter session in your browser. You can now explore and run the notebooks in the `nbs/` folder.\n",
    "\n",
    "### Demo Folders\n",
    "The demo supports a notebook with an interactive map-based walktrhough of getting AIS tracks, and querying a track for satellite coverage, using the `Skyfield` package for ephemeris calculations.  \n",
    "* `demo` - As much of the demo as possible lives under here, for completeness.\n",
    "* `data` - Daily satellite files stored (or linked) as`data/VAULT_Data/TLE_daily/`_year_`/`_MM_`/`_nn_`.tab.gz`. Used by the demo *and* other notebooks & scripts.  \n",
    "\n",
    "\n",
    "### Other folders\n",
    "* `autoencoder` - Exploratory work using a PyTorch deep network to discover high-level features and pattersn in the AIS data.\n",
    "* `ais-kml` - Concurrent visualization attempt using OpenSphere.\n",
    "* `hittestservice` - First attempt to wrap HitTest code into a web service.\n",
    "* `scripts` - A collection of scripts, esp. SQL queries.\n",
    "\n",
    "\n",
    "### A note on Spark\n",
    "Some code expects an Apache Spark setup with Hive and Hadoop available. The `ais-analytics` and `geotransformer` folders contain `cookie-cutter` setups with scripts that \n",
    "will start Spark-enabled jupyter notebooks, or launch a spark job with the required virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Required Packages\n",
    "\n",
    "The `environment.yml` file invoked above has the full package list, but key top-level packages fall in three broad categories:\n",
    "\n",
    "### Scientific Python Ecosystem:\n",
    "* Core: [numpy](https://numpy.org), [pandas](https://pandas.pydata.org)\n",
    "* Astronomy: [Skyfield](https://rhodesmill.org/skyfield/), [astropy](https://www.astropy.org), [GDAL](https://gdal.org), (pyorbital??)\n",
    "* Clustering: [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/index.html)\n",
    "* Notebooks:  [ipython](https://ipython.org), [jupyter](https://jupyter.org) notebooks.\n",
    "\n",
    "### Cloud Computing\n",
    "* [Spark](https://spark.apache.org) including PySpark, [Hive](https://hive.apache.org), [Hadoop](https://hadoop.apache.org)\n",
    "* (Other database as req'd)\n",
    "* Map support: geopandas, ...\n",
    "* Visualization: plotly, (matplotlib?), (leaflet?), (opensphere?)\n",
    "\n",
    "### Literate Programming: \n",
    "* [nbdev](https://nbdev.fast.ai), [cookie-cutter](https://cookiecutter.readthedocs.io/en/latest/README.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests are automatically extracted from notebooks in `nbs/`. To run the tests in parallel, launch:\n",
    "\n",
    "`nbdev_test_nbs` or `make test`\n",
    "\n",
    "For all the tests to pass, you'll need to install the following optional dependencies:\n",
    "\n",
    "```\n",
    "pip install ...\n",
    "```\n",
    "\n",
    "Tests are written using <code>nbdev</code>, for example see the documentation for `hit_quality` or `viz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jekyll": {
   "keywords": "fastai"
  },
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
